{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe/nKjjd0rg7hphu0QG6in",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eriksali/DNN_2023_DL/blob/main/a3_cnn_3x3_padding%3D1_5x5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1OWmhrYckcK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "## input image size = 28x28, input channel = 1, \n",
        "## filter size = 5x5, padding=2, conv (6); \n",
        "## 2x2, stride = 2, avgpool; \n",
        "## 5x5, conv (16); \n",
        "## 2x2, stride=2, avgpool;\n",
        "## fc(120, fc(84), fc(5)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# Select 5 classes: T-shirt/top, Trouser, Pullover, Dress, Coat\n",
        "selected_classes = [0, 1, 2, 3, 4]\n",
        "\n",
        "##original_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create a custom subset that contains only the selected classes\n",
        "selected_indices = []\n",
        "for i in range(len(train_dataset)):\n",
        "    _, label = train_dataset[i]\n",
        "    if label in selected_classes:\n",
        "        selected_indices.append(i)\n",
        "\n",
        "selected_train_dataset = Subset(train_dataset, selected_indices)\n",
        "\n",
        "batch_size = 64\n",
        "selected_train_loader = DataLoader(selected_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "##original_testset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create a custom subset that contains only the selected classes\n",
        "selected_indices = []\n",
        "for i in range(len(test_dataset)):\n",
        "    _, label = test_dataset[i]\n",
        "    if label in selected_classes:\n",
        "        selected_indices.append(i)\n",
        "\n",
        "selected_test_dataset = Subset(test_dataset, selected_indices)\n",
        "import random\n",
        "# Select a random subset of 100 images from the test dataset\n",
        "test_subset_indices = random.sample(range(len(selected_test_dataset)), k=100)\n",
        "test_subset = Subset(selected_test_dataset, test_subset_indices)\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "selected_test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define function to flip images horizontally or vertically\n",
        "def flip_images(images, mode='horizontal'):\n",
        "    if mode == 'horizontal':\n",
        "        return torch.flip(images, dims=[3])\n",
        "    elif mode == 'vertical':\n",
        "        return torch.flip(images, dims=[2])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode\")\n",
        "\n",
        "# Flip test images vertically and measure accuracy\n",
        "test_subset_flipped_vertical = [(flip_images(images, mode='vertical'), labels) for images, labels in test_subset]\n",
        "test_loader_flipped_vertical = torch.utils.data.DataLoader(test_subset_flipped_vertical, batch_size)\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "'''class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Define convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3, padding = 2)  # input_channel=1, output_channel=6, kernel_size=3\n",
        "        self.pool1 = nn.AvgPool2d(2, 2) # kernel_size=2, stride=2\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3) # input_channel=6, output_channel=16, kernel_size=3\n",
        "        self.pool2 = nn.AvgPool2d(2, 2) # kernel_size=2, stride=2\n",
        "        \n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # input_size=16*5*5, output_size=120\n",
        "        self.fc2 = nn.Linear(120, 84)         # input_size=120, output_size=84\n",
        "        self.fc3 = nn.Linear(84, 5)           # input_size=84, output_size=4\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolutional layers with activation functions and pooling layers\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Flatten output from convolutional layers\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        # Apply fully connected layers with activation functions\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "'''\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "'''class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3, padding=2)\n",
        "        self.pool1 = nn.AvgPool2d(2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3, padding=1)\n",
        "        self.pool2 = nn.AvgPool2d(2, stride=2)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolutional layers\n",
        "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
        "        \n",
        "        # Flatten output for fully connected layers\n",
        "        x = x.view(-1, 16 * 7 * 7)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "'''\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2)\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Convolutional layers\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Flatten the output from the convolutional layers\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(selected_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Extract the weights of the filters in both convolutional layers\n",
        "conv1_weights = net.conv1.weight.detach().clone()\n",
        "conv2_weights = net.conv2.weight.detach().clone()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the function to extract and visualize the weights of the filters\n",
        "def visualize_filters(net):\n",
        "    conv1_weights = net.conv1.weight.detach().cpu().numpy()\n",
        "    conv2_weights = net.conv2.weight.detach().cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(5,5))\n",
        "    fig.suptitle('Conv1 Weights')\n",
        "    for i in range(2):\n",
        "        for j in range(3):\n",
        "            axs[i, j].imshow(conv1_weights[i*3+j][0], cmap='gray')\n",
        "            #axs[i, j].axis('off')\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(7,7))\n",
        "    fig.suptitle('Conv2 Weights')\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            axs[i, j].imshow(conv2_weights[i*4+j][0], cmap='gray')\n",
        "            #axs[i, j].axis('off')\n",
        "\n",
        "visualize_filters(net)\n",
        "\n",
        "path = F\"/content/cnn_model_5.pt\" \n",
        "torch.save(net.state_dict(), path)\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_flipped_vertical:\n",
        "    ##for inputs, labels in selected_test_loader:\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.append(predicted)\n",
        "        true_labels.append(labels)\n",
        "\n",
        "predictions = torch.cat(predictions, dim=0)\n",
        "true_labels = torch.cat(true_labels, dim=0)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions, labels=selected_classes)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "#sns.set(font_scale=1.2)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=selected_classes, yticklabels=selected_classes)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "6Qkk9UX0P506"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErLCsZQXP7Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## input image size = 28x28, input channel = 1, \n",
        "## filter size = 5x5, padding=2, conv (6); \n",
        "## 2x2, stride = 2, avgpool; \n",
        "## 5x5, conv (16); \n",
        "## 2x2, stride=2, avgpool;\n",
        "## fc(120, fc(84), fc(5)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# Select 5 classes: T-shirt/top, Trouser, Pullover, Dress, Coat\n",
        "selected_classes = [0, 1, 2, 3, 4]\n",
        "\n",
        "##original_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create a custom subset that contains only the selected classes\n",
        "selected_indices = []\n",
        "for i in range(len(train_dataset)):\n",
        "    _, label = train_dataset[i]\n",
        "    if label in selected_classes:\n",
        "        selected_indices.append(i)\n",
        "\n",
        "selected_train_dataset = Subset(train_dataset, selected_indices)\n",
        "\n",
        "batch_size = 64\n",
        "selected_train_loader = DataLoader(selected_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "##original_testset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create a custom subset that contains only the selected classes\n",
        "selected_indices = []\n",
        "for i in range(len(test_dataset)):\n",
        "    _, label = test_dataset[i]\n",
        "    if label in selected_classes:\n",
        "        selected_indices.append(i)\n",
        "\n",
        "selected_test_dataset = Subset(test_dataset, selected_indices)\n",
        "import random\n",
        "# Select a random subset of 100 images from the test dataset\n",
        "test_subset_indices = random.sample(range(len(selected_test_dataset)), k=100)\n",
        "test_subset = Subset(selected_test_dataset, test_subset_indices)\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "selected_test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define function to flip images horizontally or vertically\n",
        "def flip_images(images, mode='horizontal'):\n",
        "    if mode == 'horizontal':\n",
        "        return torch.flip(images, dims=[3])\n",
        "    elif mode == 'vertical':\n",
        "        return torch.flip(images, dims=[2])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode\")\n",
        "\n",
        "# Flip test images vertically and measure accuracy\n",
        "test_subset_flipped_vertical = [(flip_images(images, mode='vertical'), labels) for images, labels in test_subset]\n",
        "test_loader_flipped_vertical = torch.utils.data.DataLoader(test_subset_flipped_vertical, batch_size)\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3, padding=2)\n",
        "        self.pool1 = nn.AvgPool2d(2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3, padding=1)\n",
        "        self.pool2 = nn.AvgPool2d(2, stride=2)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolutional layers\n",
        "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
        "        \n",
        "        # Flatten output for fully connected layers\n",
        "        x = x.view(-1, 16 * 7 * 7)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(selected_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Extract the weights of the filters in both convolutional layers\n",
        "conv1_weights = net.conv1.weight.detach().clone()\n",
        "conv2_weights = net.conv2.weight.detach().clone()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the function to extract and visualize the weights of the filters\n",
        "def visualize_filters(net):\n",
        "    conv1_weights = net.conv1.weight.detach().cpu().numpy()\n",
        "    conv2_weights = net.conv2.weight.detach().cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(5,5))\n",
        "    fig.suptitle('Conv1 Weights')\n",
        "    for i in range(2):\n",
        "        for j in range(3):\n",
        "            axs[i, j].imshow(conv1_weights[i*3+j][0], cmap='gray')\n",
        "            #axs[i, j].axis('off')\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(7,7))\n",
        "    fig.suptitle('Conv2 Weights')\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            axs[i, j].imshow(conv2_weights[i*4+j][0], cmap='gray')\n",
        "            #axs[i, j].axis('off')\n",
        "\n",
        "visualize_filters(net)\n",
        "\n",
        "path = F\"/content/cnn_model_5.pt\" \n",
        "torch.save(net.state_dict(), path)\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_flipped_vertical:\n",
        "    ##for inputs, labels in selected_test_loader:\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.append(predicted)\n",
        "        true_labels.append(labels)\n",
        "\n",
        "predictions = torch.cat(predictions, dim=0)\n",
        "true_labels = torch.cat(true_labels, dim=0)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions, labels=selected_classes)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "#sns.set(font_scale=1.2)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=selected_classes, yticklabels=selected_classes)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6t5C5JCe_vkC",
        "outputId": "c4c9710d-5e3a-43a8-deea-00f5d98c6ad3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 1.610\n",
            "[1,   400] loss: 1.608\n",
            "[2,   200] loss: 1.605\n",
            "[2,   400] loss: 1.600\n",
            "[3,   200] loss: 1.576\n",
            "[3,   400] loss: 1.467\n",
            "[4,   200] loss: 0.893\n",
            "[4,   400] loss: 0.638\n",
            "[5,   200] loss: 0.529\n",
            "[5,   400] loss: 0.489\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 6 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAE+CAYAAAAOBJy+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAATm0lEQVR4nO3df6zddX3H8dervb1tB/dSpBcH/QGYEbNmDVOvndPpNl0yynQwnEnB4LI4yXQwGh2MOGfEEBfJpsaM4ZgSBBXixC2SFInJzGoZMC5EGS3BlB+l/JLWWm6Lhbb0vT/OqZ62h3vOufd8v+fzvvf5SG7S873nfj/vL6/eF9/zvaf364gQAGQ2b9ADAMBMUWQA0qPIAKRHkQFIjyIDkB5FBiA9igyp2P647S93+dxP2f5a1TNh8CiyWcD2hbYnbO+1/aztO2z/TsVr/obtO23vtD3lmxFt/6vt61oeL7D94qtse8tU+4qIz0TEX8z8CCTbT9j+g37sC4NFkSVn+6OSviDpM5JeK2mlpH+RdG7FSx+Q9E1JH+ziuRslvaPl8bikJyW9/ahtknR/X6bDnEKRJWb7BEmflvRXEfHtiHgxIg5ExO0RcXnzOQttf8H2M82PL9he2Pzc79l+yvbHbD/fPJv78+bnfsv2c7bnt6z3J7YflKSIeCQiviJpcxejbpT067aXNh+/XdKtko47atvdEXHA9qm2b7O9w/bjtv+6ZYYjXi7a/oDtbbZ/avvv25xlDdu+yfYe25ttjze/7mY1Sv/25pnsFbYX2f5ac1+7bd9n+7VdB4KBochy+21JiyT9xxTP+TtJb5H0m5LOkrRG0idaPv+rkk6QtEyNs6trbZ8YEfdKelHSO1uee6Gkb/Q6ZERsl7RNvzwDe4ekH0j6n6O2bbQ9T9Ltkn7UnOldktbb/sOj92t7lRpnn++XdErLcbT6YzVKc4mk70j65+ZMF6lxVvieiDg+Iq6R9GfNfayQdJKkv5S0r9fjRf0ostxOkrQzIg5O8Zz3S/p0RDwfETskXSXpopbPH2h+/kBEbJC0V9Lrm5+7RdIFkmR7RNI5zW3T8d+S3tEsqjWS7lGjzA5ve1vzOW+WNBYRn46I/RHxmKR/k7SuzT7/VNLtEbEpIvZL+qSko6/XbYqIDRHxiqSb1SjzV3NAjf+mvxYRr0TE/RExOc3jRY0ostx+Kmmp7aEpnnOqGmdDh21rbvvFPo4qwp9LOr75529IOr/5UvR8SQ9EROu+enH4OtlqSY9FxM8lbWrZtljSvZJOk3Rq86Xdbtu7JX1cjet/7Y5t++EHzX3+9KjnPHfUsS2a4r/XzZLulHRr82X4NbYX9HaYGASKLLe7Jb0s6bwpnvOMGuVw2Mrmto4iYosaxbdW03xZ2WKjGmdDf6TGmZjUuL62orntvoh4SY1iejwilrR8jETEOW32+ayk5Ycf2F6sxhlVt444e2uelV4VEaskvVXSuyV9oIf9YUAossQi4gU1Xk5da/s827/SfBvDWtvXNJ92i6RP2B5rXlj/pKRe3lv1DUmXqXHm9O+HN7phkaTh5uNFh3+I8CqzbpX0k+a+ftDcFmqchV2mRtFJ0v9K2mP7b20vtj2/+VaPN7fZ7bckvcf2W20PS/qUJPdwbD+R9LqWY/p926ubP+CYVOOl5qEe9ocBociSi4h/kvRRNS7g71DjjOYSSf/ZfMrVkiYkPSjp/yQ90NzWrVsk/a6k/4qInS3bT1PjQvjhn1ruk/RIh31tlDQm6a6WbT+QdHLzc2pey3q3Gj+ceFzSTklfVuMi/BEiYrOkS9W4mP+sGtf3nlfjLLUb/6BGye+2/Tdq/ODjW2qU2MNqXLO7uct9YYDML1bEbGH7eEm7JZ0ZEY8PeBzUiDMypGb7Pc2X1MdJ+kc1zjqfGOxUqBtFhuzOVeOHF89IOlPSuuBlxpzDS0sA6XFGBiA9igxAehQZgPQoMgDpUWQA0qPIAKRHkQFIjyIDkB5FBiA9igxAehQZgPQoMgDpUWQA0qPIAKRHkQFIjyIDkB5FBiA9igxAehQZgPQoMgDpUWQA0qPIAKRHkQFIjyIDkB5FBiA9igxAehQZgPQoMgDpUWQA0qPIAKRHkQFIjyIDkB5FBiA9igxAehQZgPQoMgDpUWQA0qPIAKRHkQFIjyIDkB5FBiA9igxAekNV7HR0dDTGxsaq2HVbBw4cqG2tF154oba19u3bp/3793u6Xz8yMlJrDvPm1ff/xSVLltS21hNPPKGdO3dOOwdJsh32jHbRk5NOOqm2tU499dTa1tq+fbt27dp1zH/ISopsbGxMn/3sZ6vYdVvPPPNMbWt997vfrW2tu+66a0ZfPzY2pquvvrpP03S2aNGi2tY6//zza1trfHx8xvuwreHh4T5M053zzjuvtrWuuuqq2tZau3Zt2+28tASQHkUGID2KDEB6FBmA9CgyAOlRZADSo8gApEeRAUiPIgOQXldFZvts24/Y3mr7yqqHQnvkUA6yKEvHIrM9X9K1ktZKWiXpAturqh4MRyKHcpBFebo5I1sjaWtEPBYR+yXdKuncasdCG+RQDrIoTDdFtkzS9pbHTzW3HcH2xbYnbE9MTk72az78Us857Nmzp7bh5pies4iI2oabi/p2sT8iro+I8YgYHx0d7ddu0aPWHEZGRgY9zpzWmkWdv8JnLuqmyJ6WtKLl8fLmNtSLHMpBFoXppsjuk3Sm7TNsD0taJ+k71Y6FNsihHGRRmI6/WDEiDtq+RNKdkuZLuiEiNlc+GY5ADuUgi/J09RtiI2KDpA0Vz4IOyKEcZFEW3tkPID2KDEB6FBmA9CgyAOlRZADSo8gApEeRAUiPIgOQXldviO3VvHnzdNxxx1Wx67b27t1b21p33HFHbWvN1JNPPqmPfOQjta13yimn1LbWm970ptrW2r9//4z3sXTpUr33ve/twzTd+dKXvlTbWmeddVZta/3sZz9ru50zMgDpUWQA0qPIAKRHkQFIjyIDkB5FBiA9igxAehQZgPQoMgDpdXOn8RtsP2/7oToGwqsjizKQQ3m6OSO7UdLZFc+B7twosijBjSKHonQssojYKGlXDbOgA7IoAzmUh2tkANLrW5HZvtj2hO2JycnJfu0WPWrN4dChQ4MeZ05rzWLfvn2DHmdW61uRRcT1ETEeEeOjo6P92i161JrDvHmccA9SaxaLFy8e9DizGn/TAaTXzdsvbpF0t6TX237K9gerHwvtkEUZyKE8HX9DbERcUMcg6IwsykAO5eGlJYD0KDIA6VFkANKjyACkR5EBSI8iA5AeRQYgPYoMQHod3xA7HSeccILWrl1bxa7bOvHEE2tb6/LLL69trZtuumlGX//KK6/ohRde6NM0na1fv762tT784Q/Xtta2bdtmvI+VK1fquuuu68M03YmI2tZasmRJbWsNDbWvLM7IAKRHkQFIjyIDkB5FBiA9igxAehQZgPQoMgDpUWQA0qPIAKTXze/sX2H7+7a32N5s+7I6BsORyKEcZFGebv6J0kFJH4uIB2yPSLrf9vciYkvFs+FI5FAOsihMxzOyiHg2Ih5o/nmPpIclLat6MByJHMpBFuXp6RqZ7dMlvUHSvZVMg66QQznIogxdF5nt4yXdJml9REy2+fwvbg+/Y8eOfs6IFr3kUP90cwvfE+XoqshsL1AjsK9HxLfbPaf19vBjY2P9nBFNveZQ73RzC98TZenmp5aW9BVJD0fE56ofCe2QQznIojzdnJG9TdJFkt5p+4fNj3MqngvHIodykEVhOr79IiI2SXINs2AK5FAOsigP7+wHkB5FBiA9igxAehQZgPQoMgDpUWQA0qPIAKRHkQFIjyIDkJ4jov87tXdI2tbjly2VtLPvw5Rhusd2WkRM+18bTzMHafZmMZAcJL4n2uhrFpUU2XTYnpitv7Eh27Flm7db2Y4r27y96Pex8dISQHoUGYD0Siqy6wc9QIWyHVu2ebuV7biyzduLvh5bMdfIAGC6SjojA4BpKaLIbJ9t+xHbW21fOeh5+iXbjVzJoRyzMYtKc4iIgX5Imi/pUUmvkzQs6UeSVg16rj4d2ymS3tj884ikH5d6bORQzsdszaLKHEo4I1sjaWtEPBYR+yXdKuncAc/UF5HrRq7kUI5ZmUWVOZRQZMskbW95/JTK/ks2LQlu5EoO5Zj1WfQ7hxKKbNbrdCNX1IMcylBFDiUU2dOSVrQ8Xt7cNit0cyPXQpBDOWZtFlXlMPD3kdkeUuOi37vUCOs+SRdGxOaBDtYHzRu5flXSrohYP+BxpkQO5ZitWVSZw8DPyCLioKRLJN2pxsW/b2YPrEWaG7mSQzlmcRaV5TDwMzIAmKmBn5EBwExRZADSo8gApEeRAUiPIgOQHkUGID2KDEB6FBmA9CgyAOlRZADSo8gApEeRAUiPIgOQHkUGID2KDEB6FBmA9CgyAOlRZADSo8gApEeRAUiPIgOQHkUGID2KDEB6FBmA9CgyAOlRZADSo8gApEeRAUiPIgOQHkUGID2KDEB6FBmA9CgyAOlRZADSo8gApEeRAUiPIgOQHkUGID2KDEB6FBmA9CgyAOlRZADSo8gApEeRAUiPIgOQHkUGID2KDEB6FBmA9CgyAOlRZADSG6pip4sXL47R0dEqdt3W3r17a1trxYoVta313HPPaffu3Z7u1w8NDcXChQv7OdKUXnrppdrWOv3002tba8eOHZqcnJx2DpI0PDwcixYt6tdIHb388su1rXXyySfXttauXbv04osvHpNFJUU2OjqqdevWVbHrtu66667a1vr85z9f21of+tCHZvT1Cxcu1KpVq/o0TWdbtmypba1rrrmmtrWuuOKKGe9j0aJFWrNmTR+m6c6jjz5a21qXXnppbWu92vcfLy0BpEeRAUiPIgOQHkUGID2KDEB6FBmA9CgyAOlRZADS66rIbJ9t+xHbW21fWfVQaI8cykEWZelYZLbnS7pW0lpJqyRdYLu+t4tDEjmUhCzK080Z2RpJWyPisYjYL+lWSedWOxbaIIdykEVhuimyZZK2tzx+qrntCLYvtj1he2Lfvn39mg+/1HMOBw8erG24OabnLA4cOFDbcHNR3y72R8T1ETEeEeOLFy/u127Ro9YchoYq+Z0A6FJrFgsWLBj0OLNaN0X2tKTW312zvLkN9SKHcpBFYbopsvsknWn7DNvDktZJ+k61Y6ENcigHWRSm42uPiDho+xJJd0qaL+mGiNhc+WQ4AjmUgyzK09VFlIjYIGlDxbOgA3IoB1mUhXf2A0iPIgOQHkUGID2KDEB6FBmA9CgyAOlRZADSo8gApOeI6PtOh4eHo87bqG/atKm2tc4444za1pKkiDjm9vDdGh8fj4mJiX6OM6XVq1fXttZDDz1U21rSzHKQJNv9/0abwujoaG1r3XPPPbWt9b73vU8PPfTQMVlwRgYgPYoMQHoUGYD0KDIA6VFkANKjyACkR5EBSI8iA5AeRQYgvW7uNH6D7edt1/tWahyDLMpADuXp5ozsRklnVzwHunOjyKIEN4ocitKxyCJio6RdNcyCDsiiDORQHq6RAUivb0Vm+2LbE7YnDh061K/doketOezYsWPQ48xprVkMepbZrm9FFhHXR8R4RIzPm8eJ3qC05jA2Njbocea01iwGPctsR+MASK+bt1/cIuluSa+3/ZTtD1Y/FtohizKQQ3mGOj0hIi6oYxB0RhZlIIfy8NISQHoUGYD0KDIA6VFkANKjyACkR5EBSI8iA5AeRQYgvY5viJ2O+fPna2RkpIpdt3XbbbfVtlZEfXe+Hx+f2T/Re/DBB7Vy5co+TdPZnj17altr6dKlta21e/fuGe9j9erV2rBhw8yH6dLy5ctrW+uLX/xibWtNTk623c4ZGYD0KDIA6VFkANKjyACkR5EBSI8iA5AeRQYgPYoMQHoUGYD0KDIA6XVz85EVtr9ve4vtzbYvq2MwHIkcykEW5enm31oelPSxiHjA9oik+21/LyK2VDwbjkQO5SCLwnQ8I4uIZyPigeaf90h6WNKyqgfDkcihHGRRnp6ukdk+XdIbJN3b5nO/uD38wYMH+zQe2uk2h0OHDtU+21zTbRa7du2qfba5pOsis328pNskrY+IY36XRuvt4YeGKvntQFBvOcybx89yqtRLFq95zWvqH3AO6epvuu0FagT29Yj4drUj4dWQQznIoizd/NTSkr4i6eGI+Fz1I6EdcigHWZSnmzOyt0m6SNI7bf+w+XFOxXPhWORQDrIoTMeLWRGxSZJrmAVTIIdykEV5uBoMID2KDEB6FBmA9CgyAOlRZADSo8gApEeRAUiPIgOQniOi/zu1d0ja1uOXLZW0s+/DlGG6x3ZaRIxNd9Fp5iDN3iwGkoPE90Qbfc2ikiKbDtsTETE+6DmqkO3Yss3brWzHlW3eXvT72HhpCSA9igxAeiUV2fWDHqBC2Y4t27zdynZc2ebtRV+PrZhrZAAwXSWdkQHAtBRRZLbPtv2I7a22rxz0PP2S7f6H5FCO2ZhFpTlExEA/JM2X9Kik10kalvQjSasGPVefju0USW9s/nlE0o9LPTZyKOdjtmZRZQ4lnJGtkbQ1Ih6LiP2SbpV07oBn6ovIdf9DcijHrMyiyhxKKLJlkra3PH5KZf8lm5ap7n9YCHIox6zPot85lFBks16n+x+iHuRQhipyKKHInpa0ouXx8ua2WSHR/Q/JoRyzNouqchj4+8hsD6lx0e9daoR1n6QLI2LzQAfrg+b9D78qaVdErB/wOFMih3LM1iyqzGHgZ2QRcVDSJZLuVOPi3zezB9Yizf0PyaEcsziLynIY+BkZAMzUwM/IAGCmKDIA6VFkANKjyACkR5EBSI8iA5AeRQYgPYoMQHr/D5O1HQBbjH4gAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x504 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = Net()\n",
        "batch_size = 64\n",
        "summary(model, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "id": "RMMdkEt2bpSA",
        "outputId": "96135878-b2d8-4fd4-b2e1-7ed4a1f0abf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [64, 5]                   --\n",
              "Conv2d: 1-1                            [64, 6, 30, 30]           60\n",
              "AvgPool2d: 1-2                         [64, 6, 15, 15]           --\n",
              "Conv2d: 1-3                            [64, 16, 15, 15]          880\n",
              "AvgPool2d: 1-4                         [64, 16, 7, 7]            --\n",
              "Linear: 1-5                            [64, 120]                 94,200\n",
              "Linear: 1-6                            [64, 84]                  10,164\n",
              "Linear: 1-7                            [64, 5]                   425\n",
              "==========================================================================================\n",
              "Total params: 105,729\n",
              "Trainable params: 105,729\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 22.83\n",
              "==========================================================================================\n",
              "Input size (MB): 0.20\n",
              "Forward/backward pass size (MB): 4.72\n",
              "Params size (MB): 0.42\n",
              "Estimated Total Size (MB): 5.34\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = Net()\n",
        "batch_size = 64\n",
        "summary(model, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSpPrJDyBDQh",
        "outputId": "1d1c62ca-4c47-4dc2-8db7-5d875fd7c0cc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [64, 5]                   --\n",
              "Conv2d: 1-1                            [64, 6, 30, 30]           60\n",
              "AvgPool2d: 1-2                         [64, 6, 15, 15]           --\n",
              "Conv2d: 1-3                            [64, 16, 15, 15]          880\n",
              "AvgPool2d: 1-4                         [64, 16, 7, 7]            --\n",
              "Linear: 1-5                            [64, 120]                 94,200\n",
              "Linear: 1-6                            [64, 84]                  10,164\n",
              "Linear: 1-7                            [64, 5]                   425\n",
              "==========================================================================================\n",
              "Total params: 105,729\n",
              "Trainable params: 105,729\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 22.83\n",
              "==========================================================================================\n",
              "Input size (MB): 0.20\n",
              "Forward/backward pass size (MB): 4.72\n",
              "Params size (MB): 0.42\n",
              "Estimated Total Size (MB): 5.34\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## input image size = 28x28, input channel = 1, \n",
        "## filter size = 3x3, padding=2, conv (6); \n",
        "## 2x2, stride = 2, avgpool; \n",
        "## 3x3, conv (16); \n",
        "## 2x2, stride=2, avgpool;\n",
        "## fc(120, fc(84), fc(5)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# Select 5 classes: T-shirt/top, Trouser, Pullover, Dress, Coat\n",
        "selected_classes = [0, 1, 2, 3, 4]\n",
        "\n",
        "##original_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create a custom subset that contains only the selected classes\n",
        "selected_indices = []\n",
        "for i in range(len(train_dataset)):\n",
        "    _, label = train_dataset[i]\n",
        "    if label in selected_classes:\n",
        "        selected_indices.append(i)\n",
        "\n",
        "selected_train_dataset = Subset(train_dataset, selected_indices)\n",
        "\n",
        "batch_size = 64\n",
        "selected_train_loader = DataLoader(selected_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "##original_testset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Create a custom subset that contains only the selected classes\n",
        "selected_indices = []\n",
        "for i in range(len(test_dataset)):\n",
        "    _, label = test_dataset[i]\n",
        "    if label in selected_classes:\n",
        "        selected_indices.append(i)\n",
        "\n",
        "selected_test_dataset = Subset(test_dataset, selected_indices)\n",
        "import random\n",
        "# Select a random subset of 100 images from the test dataset\n",
        "test_subset_indices = random.sample(range(len(selected_test_dataset)), k=100)\n",
        "test_subset = Subset(selected_test_dataset, test_subset_indices)\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "selected_test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define function to flip images horizontally or vertically\n",
        "def flip_images(images, mode='horizontal'):\n",
        "    if mode == 'horizontal':\n",
        "        return torch.flip(images, dims=[3])\n",
        "    elif mode == 'vertical':\n",
        "        return torch.flip(images, dims=[2])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode\")\n",
        "\n",
        "# Flip test images vertically and measure accuracy\n",
        "test_subset_flipped_vertical = [(flip_images(images, mode='vertical'), labels) for images, labels in test_subset]\n",
        "test_loader_flipped_vertical = torch.utils.data.DataLoader(test_subset_flipped_vertical, batch_size)\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, padding=2)\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(in_features=13 * 6 * 6, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 13 * 6 * 6)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(selected_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        print(inputs.shape)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Extract the weights of the filters in both convolutional layers\n",
        "conv1_weights = net.conv1.weight.detach().clone()\n",
        "conv2_weights = net.conv2.weight.detach().clone()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the function to extract and visualize the weights of the filters\n",
        "def visualize_filters(net):\n",
        "    conv1_weights = net.conv1.weight.detach().cpu().numpy()\n",
        "    conv2_weights = net.conv2.weight.detach().cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(5,5))\n",
        "    fig.suptitle('Conv1 Weights')\n",
        "    for i in range(2):\n",
        "        for j in range(3):\n",
        "            axs[i, j].imshow(conv1_weights[i*3+j][0], cmap='gray')\n",
        "            #axs[i, j].axis('off')\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(7,7))\n",
        "    fig.suptitle('Conv2 Weights')\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            axs[i, j].imshow(conv2_weights[i*4+j][0], cmap='gray')\n",
        "            #axs[i, j].axis('off')\n",
        "\n",
        "visualize_filters(net)\n",
        "\n",
        "path = F\"/content/cnn_model_5.pt\" \n",
        "torch.save(net.state_dict(), path)\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader_flipped_vertical:\n",
        "    ##for inputs, labels in selected_test_loader:\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.append(predicted)\n",
        "        true_labels.append(labels)\n",
        "\n",
        "predictions = torch.cat(predictions, dim=0)\n",
        "true_labels = torch.cat(true_labels, dim=0)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions, labels=selected_classes)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "#sns.set(font_scale=1.2)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=selected_classes, yticklabels=selected_classes)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "JoIZxHKSclIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = Net()\n",
        "batch_size = 64\n",
        "summary(model, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU_CXx8E-878",
        "outputId": "68c289b9-88b2-4985-e6da-b9e1fa2b314e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [64, 5]                   --\n",
              "Conv2d: 1-1                            [64, 6, 30, 30]           60\n",
              "AvgPool2d: 1-2                         [64, 6, 15, 15]           --\n",
              "Conv2d: 1-3                            [64, 16, 15, 15]          880\n",
              "AvgPool2d: 1-4                         [64, 16, 7, 7]            --\n",
              "Linear: 1-5                            [64, 120]                 94,200\n",
              "Linear: 1-6                            [64, 84]                  10,164\n",
              "Linear: 1-7                            [64, 5]                   425\n",
              "==========================================================================================\n",
              "Total params: 105,729\n",
              "Trainable params: 105,729\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 22.83\n",
              "==========================================================================================\n",
              "Input size (MB): 0.20\n",
              "Forward/backward pass size (MB): 4.72\n",
              "Params size (MB): 0.42\n",
              "Estimated Total Size (MB): 5.34\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHpZEfnsEktu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = Net()\n",
        "batch_size = 64\n",
        "summary(model, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3nhx5Un60XO",
        "outputId": "1d91bfc6-a57c-4d3e-a4e0-bf2207d7c2da"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [64, 5]                   --\n",
              "Conv2d: 1-1                            [64, 6, 30, 30]           60\n",
              "AvgPool2d: 1-2                         [64, 6, 15, 15]           --\n",
              "Conv2d: 1-3                            [64, 16, 15, 15]          880\n",
              "AvgPool2d: 1-4                         [64, 16, 7, 7]            --\n",
              "Linear: 1-5                            [64, 120]                 94,200\n",
              "Linear: 1-6                            [64, 84]                  10,164\n",
              "Linear: 1-7                            [64, 5]                   425\n",
              "==========================================================================================\n",
              "Total params: 105,729\n",
              "Trainable params: 105,729\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 22.83\n",
              "==========================================================================================\n",
              "Input size (MB): 0.20\n",
              "Forward/backward pass size (MB): 4.72\n",
              "Params size (MB): 0.42\n",
              "Estimated Total Size (MB): 5.34\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CkOGkqcb-69q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = ConvNet()\n",
        "batch_size = 16\n",
        "summary(model, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0TrEP5x334o",
        "outputId": "404005ef-b0a8-4fd8-884c-2dcf18d22056"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [16, 5]                   --\n",
              "Conv2d: 1-1                            [16, 6, 26, 26]           60\n",
              "AvgPool2d: 1-2                         [16, 6, 13, 13]           --\n",
              "Conv2d: 1-3                            [16, 16, 11, 11]          880\n",
              "AvgPool2d: 1-4                         [16, 16, 5, 5]            --\n",
              "Linear: 1-5                            [16, 120]                 48,120\n",
              "Linear: 1-6                            [16, 84]                  10,164\n",
              "Linear: 1-7                            [16, 5]                   425\n",
              "==========================================================================================\n",
              "Total params: 59,649\n",
              "Trainable params: 59,649\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 3.29\n",
              "==========================================================================================\n",
              "Input size (MB): 0.05\n",
              "Forward/backward pass size (MB): 0.79\n",
              "Params size (MB): 0.24\n",
              "Estimated Total Size (MB): 1.08\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = Net()\n",
        "batch_size = 64\n",
        "summary(model, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyiQGdjW4TT_",
        "outputId": "39bc5f2c-1cbf-424c-8f53-8f65071f721c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [64, 5]                   --\n",
              "Conv2d: 1-1                            [64, 6, 26, 26]           60\n",
              "AvgPool2d: 1-2                         [64, 6, 13, 13]           --\n",
              "Conv2d: 1-3                            [64, 16, 11, 11]          880\n",
              "AvgPool2d: 1-4                         [64, 16, 5, 5]            --\n",
              "Linear: 1-5                            [64, 120]                 48,120\n",
              "Linear: 1-6                            [64, 84]                  10,164\n",
              "Linear: 1-7                            [64, 5]                   425\n",
              "==========================================================================================\n",
              "Total params: 59,649\n",
              "Trainable params: 59,649\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 13.17\n",
              "==========================================================================================\n",
              "Input size (MB): 0.20\n",
              "Forward/backward pass size (MB): 3.17\n",
              "Params size (MB): 0.24\n",
              "Estimated Total Size (MB): 3.61\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "#model = ConvNet()\n",
        "batch_size = 64\n",
        "summary(net, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "wdn9zrSj24xx",
        "outputId": "5c709b3e-c25e-4d8f-d9dc-1aac0689a775"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1ba2399fd858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model = ConvNet()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-7186f56aa566>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Apply convolutional layers with activation functions and pooling layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 64, 1, 28, 28]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = Net()\n",
        "batch_size = 64\n",
        "summary(model, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "id": "ZxiI9GwLMxbz",
        "outputId": "2fd649fa-62ec-4bf2-a575-d8ab357d2998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-e2f4e23003b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 400]' is invalid for input of size 36864",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-b588302d9433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     )\n\u001b[0;32m--> 218\u001b[0;31m     summary_list = forward_pass(\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mexecuted_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 1, AvgPool2d: 1, Conv2d: 1, AvgPool2d: 1]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jOCzoz3r09b0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}